{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"base_setup.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOz75MiaDUegJL1aiRKE+Vl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"a4xZTHk6WT2J"},"source":["import os\r\n","import copy\r\n","import math\r\n","import numbers\r\n","import numpy as np\r\n","from datetime import datetime\r\n","import warnings\r\n","from rl.random import OrnsteinUhlenbeckProcess\r\n","# OpenAI\r\n","import gym\r\n","import pybulletgym\r\n","#PyTorch\r\n","import torch as T\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","from torch import distributions as pyd\r\n","from torch.distributions.normal import Normal\r\n","\r\n","# Fix Numpy seed\r\n","np.random.seed(0)\r\n","# Fix Torch seed\r\n","T.manual_seed(0)\r\n","# Misc\r\n","device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\r\n","T.autograd.set_detect_anomaly(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOykPuogOEZX"},"source":["class FIFO_Buffer:\r\n","        def __init__(self, max_size, state_shape, actions_shape):\r\n","#             Internal management Variables\r\n","            self.size = max_size\r\n","            self.cnt = 0\r\n","#             Variables to store\r\n","            self.state = np.zeros((max_size, state_shape))\r\n","            self.action = np.zeros((max_size, actions_shape))\r\n","            self.reward = np.zeros((max_size, 1))\r\n","            self.next_state = np.zeros((max_size, state_shape))\r\n","            self.not_done = np.zeros((max_size, 1))\r\n","            \r\n","        def store(self, state, action, reward, next_state, done):\r\n","#             Keep index within bounds\r\n","            index = self.cnt % self.size\r\n","#             Store transition\r\n","            self.state[index] = state\r\n","            self.reward[index] = reward\r\n","            self.action[index] = action\r\n","            self.next_state[index] = next_state\r\n","            self.not_done[index] = 1. - done\r\n","#             Update counter\r\n","            self.cnt += 1\r\n","    \r\n","        def get_samples(self, batch_size):\r\n","#             Filter out empty memory locations\r\n","            size = min(self.cnt, self.size)\r\n","#             Get samples\r\n","            batch = np.random.choice(size, batch_size, replace = True)\r\n","            states = self.state[batch]\r\n","            actions = self.action[batch]\r\n","            rewards = self.reward[batch]\r\n","            next_states = self.next_state[batch]\r\n","            not_dones = self.not_done[batch]\r\n","            \r\n","            return states, actions, rewards, next_states, not_dones\r\n","        \r\n","        def get_sameples_tensor(self, batch_size):\r\n","#             Filter out empty memory locations\r\n","            size = min(self.cnt, self.size)\r\n","#             Get samples\r\n","            batch = np.random.choice(size, batch_size, replace = True)\r\n","            states = self.state[batch]\r\n","            actions = self.action[batch]\r\n","            rewards = self.reward[batch]\r\n","            next_states = self.next_state[batch]\r\n","            not_dones = self.not_done[batch]\r\n","            \r\n","            return (T.tensor(states, dtype=T.float).to(device), T.tensor(actions, dtype=T.float).to(device), \r\n","                    T.tensor(rewards, dtype=T.float).to(device), T.tensor(next_states, dtype=T.float).to(device), \r\n","                    T.tensor(not_dones, dtype=T.float).to(device)\r\n","                   )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHbHD-HBOFxB"},"source":["def default_network_initialization(layers):\r\n","    for i in range(len(layers)):\r\n","        if layers[i].out_features == 1:\r\n","            f = 1 / np.sqrt(0.003)\r\n","        else:\r\n","            f = 1 / np.sqrt(layers[i].out_features)\r\n","        T.nn.init.uniform_(layers[i].weight.data, a=-f, b=f)\r\n","        T.nn.init.uniform_(layers[i].bias.data, a=-f, b=f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfuxWiNtOHCR"},"source":["class Network(nn.Module):\r\n","    def __init__(self, input_dims, fc1_dims, fc2_dims, output_dims, name='network', chpt_dir='unknwon'):\r\n","        super(Network, self).__init__()\r\n","#         Network settings\r\n","        self.layers = nn.ModuleList().to(device)\r\n","#         Checkpoint system\r\n","        self.checkpoint_file = os.path.join(chpt_dir, name+'.h5')\r\n","        if not os.path.exists(chpt_dir):\r\n","            os.makedirs(chpt_dir)\r\n","#        Input Layer\r\n","        self.layers.append(\r\n","            nn.Linear(input_dims, fc1_dims)\r\n","        )    \r\n","#         Dense Layer\r\n","        self.layers.append(\r\n","            nn.Linear(fc1_dims, fc2_dims)\r\n","        )\r\n","#         Output Layer\r\n","        self.layers.append(\r\n","            nn.Linear(fc2_dims, output_dims)\r\n","        )\r\n","    \r\n","        self.to(device)\r\n","    \r\n","    def forward(self, input_data):\r\n","        val = F.relu(self.layers[0](input_data))\r\n","        val = F.relu(self.layers[1](val))\r\n","        val = self.layers[2](val)\r\n","        return val\r\n","    \r\n","    def save(self):\r\n","        T.save(self.state_dict(), self.checkpoint_file)\r\n","        \r\n","    def load(self):\r\n","        self.load_state_dict(T.load(self.checkpoint_file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bTvoy8N0OIJx"},"source":["class Critic_Default(Network):\r\n","    def forward(self, state, actions):\r\n","        return super().forward(T.cat([state, actions], 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iL0_AfqwOJc1"},"source":["class Actor_Default(Network):\r\n","    def __init__(self, state_dims, action_dims, fc1_dims, fc2_dims, max_action, name='', chpt_dir=''):\r\n","        super().__init__(state_dims, fc1_dims, fc2_dims, action_dims, name, chpt_dir)\r\n","        self.max_action = max_action\r\n","        \r\n","        self.to(device)\r\n","    \r\n","    def forward(self, input_data):\r\n","        return T.tanh(super().forward(input_data)) * self.max_action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfR_JHmKOKZa"},"source":["class Agent:\r\n","    def remember(self, state, action, reward, next_state, done):\r\n","        raise NotImplementedError\r\n","        \r\n","    def select_action(self, state, eval = False):\r\n","        raise NotImplementedError\r\n","    \r\n","    def train(self):\r\n","        raise NotImplementedError\r\n","        \r\n","    def save_models(self):\r\n","        print(\"Not Implemented\")\r\n","        \r\n","    def load_models(self):\r\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJuzQsnXOM9s"},"source":["%matplotlib notebook\r\n","import matplotlib.pyplot as plt\r\n","\r\n","def test_agent(env, agent, n_timesteps, visualize = True, print_only = True):\r\n","    if visualize:\r\n","        env.render()\r\n","#     Plotting\r\n","    if not print_only:\r\n","        plt_timesteps = []\r\n","        plt_rewards = []\r\n","\r\n","        fig, ax = plt.subplots(1)\r\n","        ax.set_xlabel('Training step')\r\n","        ax.set_ylabel('Reward')\r\n","        plt.ion()\r\n","        lp = ax.plot([],[])[0]\r\n","      \r\n","        fig.show()\r\n","        fig.canvas.draw()\r\n","#     Initialize\r\n","    state, done = env.reset(), False\r\n","    episode_reward = 0\r\n","    episode_timesteps = 0\r\n","    episode_num = 0\r\n","    best_reward = -99999999\r\n","\r\n","    f = open(agent.dir+\"/performance.txt\", \"w+\")\r\n","    f.write(str(f'Start_time {str(datetime.now())}\\n'))\r\n","    f.close()\r\n","    for t in range(n_timesteps):\r\n","        episode_timesteps += 1\r\n","\r\n","        action = agent.select_action(np.array(state))\r\n","        next_state, reward, done, info = env.step(action)\r\n","        agent.remember(state, action, reward, next_state, float(done))\r\n","\r\n","        episode_reward += reward\r\n","        agent.train()\r\n","        state = next_state\r\n","\r\n","        if done:\r\n","          log = f\"{datetime.now()} Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\"\r\n","          # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\r\n","          if not print_only:\r\n","              plt_timesteps.append(t+1)\r\n","              plt_rewards.append(episode_reward)\r\n","              lp.set_xdata(plt_timesteps)\r\n","              lp.set_ydata(plt_rewards)\r\n","              ax.relim()\r\n","              ax.autoscale_view(True, True, True)\r\n","              fig.canvas.draw()\r\n","          else:\r\n","              print(log)\r\n","          f = open(agent.dir+\"/performance.txt\", \"a\")\r\n","          f.write(f'{log}\\n')\r\n","          f.close()\r\n","          # Reset environment\r\n","          if reward > best_reward:\r\n","              best_reward = reward\r\n","              agent.save_models()\r\n","          \r\n","          state, done = env.reset(), False\r\n","          episode_reward = 0\r\n","          episode_timesteps = 0\r\n","          episode_num += 1\r\n","    agent.save_models()\r\n","    print(datetime.now())\r\n","    print(\"END\")"],"execution_count":null,"outputs":[]}]}